{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC_n5cbrjOhb"
      },
      "source": [
        "# **LinkedIn Job Scraper for Panama City**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94Eo97_OCJRR",
        "outputId": "e298108e-f489-4ae8-a610-87d497911e9d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Function to generate LinkedIn job search URL with pagination\n",
        "def obtener_url(position, location, start=0):\n",
        "    return f\"https://www.linkedin.com/jobs/search/?keywords={position}&location={location}&start={start}\"\n",
        "\n",
        "# Function to clean extracted text\n",
        "def limpiar(texto):\n",
        "    return re.sub(r'\\s+', ' ', texto).strip() if texto else 'Not available'\n",
        "\n",
        "# Function to extract job details safely\n",
        "def obtener_data(job):\n",
        "    try:\n",
        "        location = limpiar(job.find('span', class_='job-search-card__location').get_text())\n",
        "        title = limpiar(job.find('h3', class_='base-search-card__title').get_text())\n",
        "        company = limpiar(job.find('h4', class_='base-search-card__subtitle').get_text())\n",
        "\n",
        "        # Extract job URL\n",
        "        job_link = job.find('a', class_='base-card__full-link')\n",
        "        job_url = limpiar(job_link.get('href')) if job_link else 'Not available'\n",
        "\n",
        "        return {\n",
        "            \"Location\": location,\n",
        "            \"Job Title\": title,\n",
        "            \"Company\": company,\n",
        "            \"URL\": job_url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting job details: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape LinkedIn jobs for a given city\n",
        "def codigo_principal(position, location):\n",
        "    lista_trabajos = []\n",
        "    start = 0\n",
        "\n",
        "    while True:\n",
        "        url = obtener_url(position, location, start)\n",
        "\n",
        "        # Headers to avoid blocking\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 \"\n",
        "                          \"(KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request error: {e}\")\n",
        "            break  # Stop if request fails\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find job list\n",
        "        joblist = soup.find('ul', class_='jobs-search__results-list')\n",
        "        if not joblist:\n",
        "            print(f\"No more jobs found. Stopping pagination.\")\n",
        "            break\n",
        "\n",
        "        all_jobs = joblist.find_all('li')\n",
        "\n",
        "        # If no jobs are found on this page, stop\n",
        "        if len(all_jobs) == 0:\n",
        "            break\n",
        "\n",
        "        print(f\"Found {len(all_jobs)} jobs on page {start // 25 + 1}\")\n",
        "\n",
        "        for job in all_jobs:\n",
        "            job_data = obtener_data(job)\n",
        "            if job_data:\n",
        "                lista_trabajos.append(job_data)\n",
        "\n",
        "        # Stop if the number of jobs is less than 25 (indicating last page)\n",
        "        if len(all_jobs) < 25:\n",
        "            break\n",
        "\n",
        "        start += 25  # Move to next page\n",
        "        time.sleep(3)  # Prevent getting blocked\n",
        "\n",
        "    return lista_trabajos\n",
        "\n",
        "# Run the scraper for a specific city\n",
        "position = \"data scientist\"\n",
        "location = \"Panama City, PanamÃ¡, Panama\"\n",
        "trabajos = codigo_principal(position, location)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
